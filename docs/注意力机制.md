# 注意力机制

- 卷积、全连接、池化层都只考虑不随意线索
- 注意力机制会显式考虑随意线索
    - 随意线索被称之为查询（Query）
    - 每个输入是一个值（Value）
    - 每个值对应一个不随意线索（Key）
    - 通过注意力池化层来有偏向性地选择输入

![](https://zh-v2.d2l.ai/_images/qkv.svg)

## 非参注意力池化层

- 对于给定数据集 \(\{(x_i, y_i)\}_{i=1}^n\)，其中 \(x_i \) 是key，\(y_i\) 是对应的value
- 平均池化是最简单的方案
    \[    f(x) = \frac{1}{n}\sum_{i=1}^n y_i    \]

- 加权池化(60年代提出来的Nadaraya-Watson**核回归**估计器)
    \[    f(x) = \sum_{i=1}^n {\frac{\text{K}(x, x_i)}{\sum_{j=1}^n \text{K}(x, x_j)}y_i}    \]
  
- $x$是查询(query)
- \(K\)函数(Kernel)是用来衡量两个变量之间的“距离”的函数。函数值越大，表示两个变量越相似，权重越大。所有权重加起来为1。
    - 常用的相似度函数有点积、余弦相似度、高斯核函数等

### Nadaraya-Watson核回归

使用高斯核函数作为相似度函数
\[K(u) = \frac{1}{\sqrt{2\pi}}\exp(-\frac{u^2}{2})\]

带入
\[f(x) = \sum_{i=1}^n {\frac{\exp(-\frac{(x-x_i)^2}{2})}{\sum_{j=1}^n \exp(-\frac{(x-x_j)^2}{2})}y_i} = \sum_{i=1}^n{\mathrm{softmax}(-\frac{(x-x_i)^2}{2})y_i}\]

## 参数化的注意力池化层
可以在上面的基础上引入可学习参数$w$
\[f(x) =  = \sum_{i=1}^n{\mathrm{softmax}(-\frac{w (x - x_i)^2}{2})y_i}\]

- 通过学习参数\(w\)来调整kernel的“宽度”

### 总结
加权池化可以一般地写作
\[f(x) = \sum_{i=1}^n{\alpha(x,x_i)y_i}\]
这里\(\alpha(x,x_i)\)是一个可微分的注意力权重函数，满足\(\sum_{i=1}^n \alpha(x,x_i) = 1\)

哪些是可训练参数，哪些是超参数？
- 非参注意力池化层没有可训练参数，kernel函数的选择是超参数
- 参数化注意力池化层中kernel函数中的参数是可训练参数，kernel函数的选择是超参数
- 查询\(x\)不是参数，而是输入

> - 核回归有点像KNN（K nearest neighbors，K近邻）？
> - 核回归和KNN都是基于邻近样本进行预测的非参数方法，但核回归使用加权平均，而KNN使用简单平均。

## 注意力分数
怎么设计注意力权重，怎么评价注意力机制的好坏？

![](https://zh-v2.d2l.ai/_images/attention-output.svg)

$$f(x) = \sum_{i=1}^n{\alpha(x,x_i)y_i}$$
其中，
$$\alpha(x,x_i) = \mathrm{softmax}(a(x,x_i))$$
这里的\(a(x,x_i)\)称为注意力分数(Attention Score)，表示查询\(x\)和键\(x_i\)之间的相似度。
- 比如上一节中用到的注意力分数
  \[a(x,x_i) = -\frac{(x - x_i)^2}{2}\]

怎么样把query拖展到高维度。

- 假设query $\mathbf{q} \in \mathbb{R}^{q}$, key $\mathbf{k}_i \in \mathbb{R}^{k}$, value $\mathbf{v}_i \in \mathbb{R}^{v}$,均为向量，维数可以不一样。
- 注意力池化层：
\[f(\mathbf{q},(\mathbf{k}_1,\mathbf{v}_1)...,(\mathbf{k}_m,\mathbf{v}_m)) = \sum_{i=1}^m{\alpha(\mathbf{q},\mathbf{k}_i)\mathbf{v}_i} \in \mathbb{R}^{v}\]
- $\alpha(\mathbf{q},\mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q},\mathbf{k}_i)) \in \mathbb{R}$

> 维数不一样，怎么计算注意力分数？

### 加性注意力分数（Additive Attention Score）
可学习参数：权重矩阵$\mathbf{W}_q \in \mathbb{R}^{h \times q}$, $\mathbf{W}_k \in \mathbb{R}^{h \times k}$, 向量$\mathbf{w}_a \in \mathbb{R}^{h}$, 偏置向量$\mathbf{b}_a \in \mathbb{R}^{h}$

\[a(\mathbf{q},\mathbf{k}_i) = \mathbf{w}_a^\top \tanh(\mathbf{W}_q \mathbf{q} + \mathbf{W}_k \mathbf{k}_i + \mathbf{b}_a)\]
- 等价于将query和key连结起来，经过一个隐藏大小为h，输出大小为1的MLP，得到注意力分数

### 缩放点积注意力分数（Scaled Dot-Product Attention Score）
假如query和$key$的维数相同，即$q = k = d$
\[a(\mathbf{q},\mathbf{k}_i) = \frac{\mathbf{q}^\top \mathbf{k}_i}{\sqrt{d}}\]
- 计算query和key的点积，然后除以$\sqrt{d}$进行缩放，防止点积值过大导致softmax梯度过小

**向量化版本，n个query和m个key**
- query矩阵$\mathbf{Q} \in \mathbb{R}^{n \times d}$
- key矩阵$\mathbf{K} \in \mathbb{R}^{m \times d}$
- value矩阵$\mathbf{V} \in \mathbb{R}^{m \times v}$
- 注意力分数：
\[\mathbf{A}(\mathbf{Q},\mathbf{K}) = \frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d}} \in \mathbb{R}^{n \times m}\]

- 注意力权重：$ f(\mathbf{Q},\mathbf{K},\mathbf{V}) = \mathrm{softmax}(\mathbf{A}(\mathbf{Q},\mathbf{K}))\mathbf{V} \in \mathbb{R}^{n \times v}$
- 这里softmax是对每一行进行归一化

### 总结
两种常见的分数计算
- 加性注意力分数：将query和key合并起来，通过一个单隐藏层的MLP计算query和key的相似度
- 缩放点积注意力分数：直接将query和key做内积，并进行缩放。通过点积计算query和key的相似度

## 使用注意力机制的seq2seq模型
- 一般的seq2seq模型使用编码器-解码器架构
- 编码器将输入序列编码成一个固定长度的向量
- 解码器根据该向量生成输出序列
- 这种方法在处理长序列时效果不佳，因为固定长度的向量可能无法捕捉输入序列中的所有信息
- 注意力机制通过让解码器在每个时间步动态地关注输入序列的不同部分，解决了这个问题

![](https://zh-v2.d2l.ai/_images/seq2seq-attention-details.svg)

- 编码器将输入序列编码成一组key-value对
- 解码器在每个时间步生成查询
- 通过注意力池化层计算上下文向量
- 将上下文向量和解码器隐藏状态结合起来生成输出


>【【官方双语】直观解释注意力机制，Transformer的核心 | 【深度学习第6章】】 https://www.bilibili.com/video/BV1TZ421j7Ke/?share_source=copy_web&vd_source=0bc88e723ead812499d06d8ad39e696b

## 自注意力机制
- 自注意力机制(Self-Attention)是一种特殊的注意力机制，其中查询、键和值都来自同一序列
![alt text](image.png)

### 位置编码
- 自注意力机制本身不包含位置信息，因此需要引入位置编码(Position Encoding)来捕捉序列中元素的顺序信息
- 位置编码可以是固定的（如正弦和余弦函数）
$ PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}}) \\
PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}}) $

## Transformer
### 基本架构
![alt text](image-1.png)

